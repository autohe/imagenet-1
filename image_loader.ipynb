{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "import threading\n",
    "import string\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class JpegCoder(object):\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self._cmyk_data = tf.placeholder(dtype=tf.string)\n",
    "        image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n",
    "        self._cmyk_to_rgb = tf.image.encode_jpeg(image, format='rgb', quality=100)\n",
    "\n",
    "        self.decode_jpeg_data = tf.placeholder(dtype = tf.string)\n",
    "        self.decode_jpeg = tf.image.decode_jpeg(self.decode_jpeg_data, channels = 3)\n",
    "        \n",
    "    def decoder_jpeg(self, image_data):\n",
    "        image = self.sess.run(self.decode_jpeg, feed_dict = {self.decode_jpeg_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "    \n",
    "    def cmyk_to_rgb(self, image_data):\n",
    "        return self.sess.run(self._cmyk_to_rgb,\n",
    "                          feed_dict={self._cmyk_data: image_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _is_cmyk(filename):\n",
    "    \"\"\"Determine if file contains a CMYK JPEG format image.\n",
    "    Args:\n",
    "    filename: string, path of the image file.\n",
    "    Returns:\n",
    "    boolean indicating if the image is a JPEG encoded with CMYK color space.\n",
    "    \"\"\"\n",
    "    # File list from:\n",
    "    # https://github.com/cytsai/ilsvrc-cmyk-image-list\n",
    "    blacklist = ['n01739381_1309.JPEG', 'n02077923_14822.JPEG',\n",
    "               'n02447366_23489.JPEG', 'n02492035_15739.JPEG',\n",
    "               'n02747177_10752.JPEG', 'n03018349_4028.JPEG',\n",
    "               'n03062245_4620.JPEG', 'n03347037_9675.JPEG',\n",
    "               'n03467068_12171.JPEG', 'n03529860_11437.JPEG',\n",
    "               'n03544143_17228.JPEG', 'n03633091_5218.JPEG',\n",
    "               'n03710637_5125.JPEG', 'n03961711_5286.JPEG',\n",
    "               'n04033995_2932.JPEG', 'n04258138_17003.JPEG',\n",
    "               'n04264628_27969.JPEG', 'n04336792_7448.JPEG',\n",
    "               'n04371774_5854.JPEG', 'n04596742_4225.JPEG',\n",
    "               'n07583066_647.JPEG', 'n13037406_4650.JPEG']\n",
    "    return filename.split('/')[-1] in blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(filename, coder):\n",
    "    image_data = tf.gfile.FastGFile(filename, 'rb').read()\n",
    "    \n",
    "    if _is_cmyk(filename):\n",
    "        image_data = coder.cmyk_to_rgb(image_data)\n",
    "    \n",
    "    image = coder.decoder_jpeg(image_data)\n",
    "    \n",
    "    assert len(image.shape) == 3\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    assert image.shape[2] ==3\n",
    "    \n",
    "    return image_data, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_image_batch(coder, name, thread_index, ranges, synsets, filenames, labels, num_shards):\n",
    "    num_threads = len(ranges)\n",
    "    assert not num_shards % num_threads\n",
    "    shards_in_batch = int(num_shards / num_threads)\n",
    "    \n",
    "    shard_ranges = np.linspace(ranges[thread_index][0], \n",
    "                               ranges[thread_index][1], shards_in_batch + 1).astype(int)\n",
    "    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(shards_in_batch):\n",
    "        shard = thread_index * shards_in_batch + i\n",
    "        output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)\n",
    "        output_file = os.path.join('/data/ImageNet/output_data/', output_filename)\n",
    "        writer = tf.python_io.TFRecordWriter(output_file)\n",
    "        \n",
    "        shard_counter = 0\n",
    "        files_in_shard = np.arange(shard_ranges[i], shard_ranges[i+1], dtype=int)\n",
    "        for j in files_in_shard:\n",
    "\n",
    "            filename = filenames[j]\n",
    "            label = labels[j]\n",
    "            synset = synsets[j]\n",
    "            \n",
    "            image_buffer, height, width = process_image(filename, coder)\n",
    "            \n",
    "            colorspace = b'RGB'\n",
    "            channels = 3\n",
    "            image_format = b'JPEG'\n",
    "            basename = str.encode(os.path.basename(filename))\n",
    "            synset = str.encode(synset)\n",
    "            \n",
    "            example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "                'image/height': _int64_feature(height),\n",
    "                'image/width': _int64_feature(width),\n",
    "                'image/colorspace': _bytes_feature(colorspace),\n",
    "                'image/channels': _int64_feature(channels),\n",
    "                'image/class/label': _int64_feature(label),\n",
    "                'image/class/synsets': _bytes_feature(synset),\n",
    "                'image/format': _bytes_feature(image_format),\n",
    "                'image/filename': _bytes_feature(basename),\n",
    "                'image/encoded': _bytes_feature(image_buffer)}))\n",
    "                        \n",
    "              \n",
    "            writer.write(example.SerializeToString())\n",
    "            shard_counter += 1\n",
    "            counter += 1\n",
    "            if not counter % 1000:\n",
    "                sys.stdout.flush()\n",
    "        sys.stdout.flush()    \n",
    "        shard_counter = 0\n",
    "    sys.stdout.flush()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_file_data(data_dir, chal_synsets):\n",
    "    label_index = 1\n",
    "    labels = []\n",
    "    synsets  = []\n",
    "    filenames = []\n",
    "\n",
    "    for synset in chal_synsets:\n",
    "        synset = synset.strip()\n",
    "        jpeg_path = data_dir + '%s/*.JPEG'  % synset\n",
    "        matching_files = glob.glob(jpeg_path)\n",
    "\n",
    "        labels.extend([label_index] * len(matching_files))\n",
    "        synsets.extend([synset] * len(matching_files))\n",
    "        filenames.extend(matching_files)\n",
    "\n",
    "        label_index += 1\n",
    "\n",
    "    shuffled_index = range(len(filenames))\n",
    "    random.seed(12345)\n",
    "    random.shuffle(filenames)    \n",
    "\n",
    "    filenames = [filenames[i] for i in shuffled_index]\n",
    "    synsets = [synsets[i] for i in shuffled_index]\n",
    "    labels = [labels[i] for i in shuffled_index]\n",
    "    \n",
    "    return filenames, synsets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# eval_labels = [l.strip() for l in open('/data/ImageNet/dev_kit/imagenet_2012_validation_synset_labels.txt')\n",
    "#               .readlines()]\n",
    "chal_synsets = []\n",
    "enc = 'utf-8'\n",
    "\n",
    "f = open('/data/ImageNet/dev_kit/traindata_labels.txt', 'r')\n",
    "\n",
    "chal_synsets = f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "data_dir_eval = '/data/ImageNet/eval_data/'\n",
    "data_dir_train = '/data/ImageNet/train_data/'\n",
    "\n",
    "filenames, synsets, labels = get_file_data(data_dir_train, chal_synsets)\n",
    "eval_filenames, eval_synsets, eval_labels = get_file_data(data_dir_eval, chal_synsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_shards = 1024\\neval_shards = 128\\nnum_threads = 16\\n\\nspacing = np.linspace(0, len(filenames), num_threads + 1).astype(np.int)\\nranges = []\\nthreads = []\\nfor j in range(len(spacing) - 1):\\n    ranges.append([spacing[j], spacing[j+1]])\\n\\n# launch thread\\nsys.stdout.flush()\\n\\n# monitoring\\ncoord = tf.train.Coordinator()\\n\\ncoder = JpegCoder()\\n\\nthreads = []\\nfor thread_index in range(len(ranges)):\\n    args = (coder, 'train', thread_index, ranges, synsets, filenames, labels, train_shards)\\n    t = threading.Thread(target = process_image_batch, args=args)\\n    t.start()\\n    threads.append(t)\\n    \\n    coord.join(threads)\\n    sys.stdout.flush()\\n\\nspacing = np.linspace(0, len(eval_filenames), num_threads + 1).astype(np.int)\\nranges = []\\nthreads = []\\nfor j in range(len(spacing) - 1):\\n    ranges.append([spacing[j], spacing[j+1]])\\n\\n# launch thread\\nsys.stdout.flush()\\n\\n# monitoring\\ncoord = tf.train.Coordinator()\\n\\ncoder = JpegCoder()\\n\\nthreads = []\\nfor thread_index in range(len(ranges)):\\n    args = (coder, 'eval', thread_index, ranges, eval_synsets, eval_filenames, eval_labels, eval_shards)\\n    t = threading.Thread(target = process_image_batch, args=args)\\n    t.start()\\n    threads.append(t)\\n    \\n    coord.join(threads)\\n    sys.stdout.flush()\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_shards = 1024\n",
    "eval_shards = 128\n",
    "num_threads = 16\n",
    "\n",
    "spacing = np.linspace(0, len(filenames), num_threads + 1).astype(np.int)\n",
    "ranges = []\n",
    "threads = []\n",
    "for j in range(len(spacing) - 1):\n",
    "    ranges.append([spacing[j], spacing[j+1]])\n",
    "\n",
    "# launch thread\n",
    "sys.stdout.flush()\n",
    "\n",
    "# monitoring\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "coder = JpegCoder()\n",
    "\n",
    "threads = []\n",
    "for thread_index in range(len(ranges)):\n",
    "    args = (coder, 'train', thread_index, ranges, synsets, filenames, labels, train_shards)\n",
    "    t = threading.Thread(target = process_image_batch, args=args)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "    coord.join(threads)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "spacing = np.linspace(0, len(eval_filenames), num_threads + 1).astype(np.int)\n",
    "ranges = []\n",
    "threads = []\n",
    "for j in range(len(spacing) - 1):\n",
    "    ranges.append([spacing[j], spacing[j+1]])\n",
    "\n",
    "# launch thread\n",
    "sys.stdout.flush()\n",
    "\n",
    "# monitoring\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "coder = JpegCoder()\n",
    "\n",
    "threads = []\n",
    "for thread_index in range(len(ranges)):\n",
    "    args = (coder, 'eval', thread_index, ranges, eval_synsets, eval_filenames, eval_labels, eval_shards)\n",
    "    t = threading.Thread(target = process_image_batch, args=args)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "    coord.join(threads)\n",
    "    sys.stdout.flush()\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"A simple class for handling data sets.\"\"\"\n",
    "\n",
    "    def __init__(self,  subset):\n",
    "        \"\"\"Initialize dataset using a subset and the path to the data.\"\"\"\n",
    "        assert subset in self.available_subsets(), self.available_subsets()\n",
    "        self.subset = subset\n",
    "\n",
    "    def num_classes(self):\n",
    "        return 1000\n",
    "\n",
    "    def num_examples_per_epoch(self):\n",
    "        \"\"\"Returns the number of examples in the data subset.\"\"\"\n",
    "        if self.subset == 'train':\n",
    "            return 1281167\n",
    "        if self.subset == 'eval':\n",
    "            return 50000\n",
    "\n",
    "\n",
    "    def available_subsets(self):\n",
    "        \"\"\"Returns the list of available subsets.\"\"\"\n",
    "        return ['train', 'eval']\n",
    "\n",
    "    def data_files(self):\n",
    "        \"\"\"Returns a python list of all (sharded) data subset files.\n",
    "        Returns:\n",
    "          python list of all (sharded) data set files.\n",
    "        Raises:\n",
    "          ValueError: if there are not data_files matching the subset.\n",
    "        \"\"\"\n",
    "        tf_record_pattern = os.path.join('/data/ImageNet/output_data/', '%s-*' % self.subset)\n",
    "        data_files = tf.gfile.Glob(tf_record_pattern)\n",
    "        if not data_files:\n",
    "            print('No files found for dataset %s' % (self.subset))\n",
    "\n",
    "            exit(-1)\n",
    "        return data_files\n",
    "\n",
    "    def reader(self):\n",
    "        \"\"\"Return a reader for a single entry from the data set.\n",
    "        See io_ops.py for details of Reader class.\n",
    "        Returns:\n",
    "          Reader object that reads the data set.\n",
    "        \"\"\"\n",
    "        return tf.TFRecordReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jpeg_to_tensor(image_buffer, scope=None):\n",
    "    \"\"\"Decode a JPEG string into one 3-D float image Tensor.\n",
    "    Args:\n",
    "    image_buffer: scalar string Tensor.\n",
    "    scope: Optional scope for op_scope.\n",
    "    Returns:\n",
    "    3-D float Tensor with values ranging from [0, 1).\n",
    "    \"\"\"\n",
    "    with tf.op_scope([image_buffer], scope, 'decode_jpeg'):\n",
    "        # Decode the string as an RGB JPEG.\n",
    "        # Note that the resulting image contains an unknown height and width\n",
    "        # that is set dynamically by decode_jpeg. In other words, the height\n",
    "        # and width of image is unknown at compile-time.\n",
    "        image = tf.image.decode_jpeg(image_buffer, channels=3)\n",
    "\n",
    "        # After this point, all image pixels reside in [0,1)\n",
    "        # until the very end, when they're rescaled to (-1, 1).  The various\n",
    "        # adjust_* ops all require this range for dtype float.\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        return image\n",
    "\n",
    "\n",
    "def distort_color(image, thread_id=0, scope=None):\n",
    "    \"\"\"Distort the color of the image.\n",
    "    Each color distortion is non-commutative and thus ordering of the color ops\n",
    "    matters. Ideally we would randomly permute the ordering of the color ops.\n",
    "    Rather then adding that level of complication, we select a distinct ordering\n",
    "    of color ops for each preprocessing thread.\n",
    "    Args:\n",
    "    image: Tensor containing single image.\n",
    "    thread_id: preprocessing thread ID.\n",
    "    scope: Optional scope for op_scope.\n",
    "    Returns:\n",
    "    color-distorted image\n",
    "    \"\"\"\n",
    "    with tf.op_scope([image], scope, 'distort_color'):\n",
    "        color_ordering = thread_id % 2\n",
    "\n",
    "        if color_ordering == 0:\n",
    "            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "            image = tf.image.random_hue(image, max_delta=0.2)\n",
    "            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        elif color_ordering == 1:\n",
    "            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "            image = tf.image.random_hue(image, max_delta=0.2)\n",
    "\n",
    "        # The random_* ops do not necessarily clamp.\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "        return image\n",
    "\n",
    "\n",
    "def distort_image(image, height, width, thread_id=0, scope=None):\n",
    "    \"\"\"Distort one image for training a network.\n",
    "    Distorting images provides a useful technique for augmenting the data\n",
    "    set during training in order to make the network invariant to aspects\n",
    "    of the image that do not effect the label.\n",
    "    Args:\n",
    "    image: 3-D float Tensor of image\n",
    "    height: integer\n",
    "    width: integer\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged\n",
    "      as [ymin, xmin, ymax, xmax].\n",
    "    thread_id: integer indicating the preprocessing thread.\n",
    "    scope: Optional scope for op_scope.\n",
    "    Returns:\n",
    "    3-D float Tensor of distorted image used for training.\n",
    "    \"\"\"\n",
    "    distorted_image = image\n",
    "\n",
    "    # This resizing operation may distort the images because the aspect\n",
    "    # ratio is not respected. We select a resize method in a round robin\n",
    "    # fashion based on the thread number.\n",
    "    # Note that ResizeMethod contains 4 enumerated resizing methods.\n",
    "    resize_method = thread_id % 4\n",
    "    distorted_image = tf.image.resize_images(distorted_image, height, width,\n",
    "                                             resize_method)\n",
    "    # Restore the shape since the dynamic slice based upon the bbox_size loses\n",
    "    # the third dimension.\n",
    "    distorted_image.set_shape([height, width, 3])\n",
    "    if not thread_id:\n",
    "        tf.image_summary('cropped_resized_image',\n",
    "                       tf.expand_dims(distorted_image, 0))\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "    # Randomly distort the colors.\n",
    "    distorted_image = distort_color(distorted_image, thread_id)\n",
    "\n",
    "    if not thread_id:\n",
    "        tf.image_summary('final_distorted_image',\n",
    "                       tf.expand_dims(distorted_image, 0))\n",
    "    return distorted_image\n",
    "\n",
    "\n",
    "def eval_image(image, height, width, scope=None):\n",
    "    \"\"\"Prepare one image for evaluation.\n",
    "    Args:\n",
    "    image: 3-D float Tensor\n",
    "    height: integer\n",
    "    width: integer\n",
    "    scope: Optional scope for op_scope.\n",
    "    Returns:\n",
    "    3-D float Tensor of prepared image.\n",
    "    \"\"\"\n",
    "    with tf.op_scope([image, height, width], scope, 'eval_image'):\n",
    "        # Crop the central region of the image with an area containing 87.5% of\n",
    "        # the original image.\n",
    "        image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "\n",
    "        # Resize the image to the original height and width.\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.image.resize_bilinear(image, [height, width],\n",
    "                                         align_corners=False)\n",
    "        image = tf.squeeze(image, [0])\n",
    "        return image\n",
    "\n",
    "\n",
    "def image_preprocessing(image_buffer, train, thread_id=0):\n",
    "    \"\"\"Decode and preprocess one image for evaluation or training.\n",
    "    Args:\n",
    "    image_buffer: JPEG encoded string Tensor\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged as\n",
    "      [ymin, xmin, ymax, xmax].\n",
    "    train: boolean\n",
    "    thread_id: integer indicating preprocessing thread\n",
    "    Returns:\n",
    "    3-D float Tensor containing an appropriately scaled image\n",
    "    Raises:\n",
    "    ValueError: if user does not provide bounding box\n",
    "    \"\"\"\n",
    "    \n",
    "    image = jpeg_to_tensor(image_buffer)\n",
    "    height = 224\n",
    "    width = 224\n",
    "\n",
    "    if train:\n",
    "        image = distort_image(image, height, width, thread_id)\n",
    "    else:\n",
    "        image = eval_image(image, height, width)\n",
    "\n",
    "    # Finally, rescale to [-1,1] instead of [0, 1)\n",
    "    image = tf.sub(image, 0.5)\n",
    "    image = tf.mul(image, 2.0)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_example_proto(example_serialized):\n",
    "    \"\"\"Parses an Example proto containing a training example of an image.\n",
    "    The output of the build_image_data.py image preprocessing script is a dataset\n",
    "    containing serialized Example protocol buffers. Each Example proto contains\n",
    "    the following fields:\n",
    "    image/height: 462\n",
    "    image/width: 581\n",
    "    image/colorspace: 'RGB'\n",
    "    image/channels: 3\n",
    "    image/class/label: 615\n",
    "\n",
    "\n",
    "    image/format: 'JPEG'\n",
    "    image/filename: 'ILSVRC2012_val_00041207.JPEG'\n",
    "    image/encoded: <JPEG encoded string>\n",
    "    Args:\n",
    "    example_serialized: scalar Tensor tf.string containing a serialized\n",
    "      Example protocol buffer.\n",
    "    Returns:\n",
    "    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n",
    "    label: Tensor tf.int32 containing the label.\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged as\n",
    "      [ymin, xmin, ymax, xmax].\n",
    "    text: Tensor tf.string containing the human-readable label.\n",
    "    \"\"\"\n",
    "    # Dense features in Example proto.\n",
    "    feature_map = {\n",
    "      'image/encoded': tf.FixedLenFeature([], dtype=tf.string,\n",
    "                                          default_value=''),\n",
    "      'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64,\n",
    "                                              default_value=-1),\n",
    "      'image/class/synset': tf.FixedLenFeature([], dtype=tf.string, default_value='')\n",
    "\n",
    "    }\n",
    "    sparse_float32 = tf.VarLenFeature(dtype=tf.float32)\n",
    "    # Sparse features in Example proto.\n",
    "\n",
    "    features = tf.parse_single_example(example_serialized, feature_map)\n",
    "    label = tf.cast(features['image/class/label'], dtype=tf.int32)\n",
    "\n",
    "\n",
    "\n",
    "    return features['image/encoded'], label, features['image/class/synset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_inputs(dataset, batch_size, train, num_preprocess_threads, num_readers):\n",
    "    data_files = dataset.data_files()\n",
    "    if data_files is None:\n",
    "        raise ValueError('No data files found for this dataset')\n",
    "\n",
    "    # Create filename_queue\n",
    "    if train:\n",
    "        filename_queue = tf.train.string_input_producer(data_files,\n",
    "                                                      shuffle=True,\n",
    "                                                      capacity=16)\n",
    "    else:\n",
    "        filename_queue = tf.train.string_input_producer(data_files,\n",
    "                                                      shuffle=False,\n",
    "                                                      capacity=1)\n",
    "    if num_preprocess_threads is None:\n",
    "        num_preprocess_threads = FLAGS.num_preprocess_threads\n",
    "\n",
    "    if num_preprocess_threads % 4:\n",
    "        raise ValueError('Please make num_preprocess_threads a multiple '\n",
    "                       'of 4 (%d % 4 != 0).', num_preprocess_threads)\n",
    "\n",
    "    if num_readers is None:\n",
    "        num_readers = FLAGS.num_readers\n",
    "\n",
    "    if num_readers < 1:\n",
    "        raise ValueError('Please make num_readers at least 1')\n",
    "        \n",
    "    examples_per_shard = 1024\n",
    "    \n",
    "    min_queue_examples = examples_per_shard * 16\n",
    "    if train:\n",
    "        examples_queue = tf.RandomShuffleQueue(\n",
    "              capacity=min_queue_examples + 3 * batch_size,\n",
    "              min_after_dequeue=min_queue_examples,\n",
    "              dtypes=[tf.string])\n",
    "    else:\n",
    "        examples_queue = tf.FIFOQueue(\n",
    "              capacity=examples_per_shard + 3 * batch_size,\n",
    "              dtypes=[tf.string])\n",
    "\n",
    "    # Create multiple readers to populate the queue of examples.\n",
    "    if num_readers > 1:\n",
    "        enqueue_ops = []\n",
    "        for _ in range(num_readers):\n",
    "            reader = dataset.reader()\n",
    "            _, value = reader.read(filename_queue)\n",
    "            enqueue_ops.append(examples_queue.enqueue([value]))\n",
    "\n",
    "            tf.train.queue_runner.add_queue_runner(\n",
    "                    tf.train.queue_runner.QueueRunner(examples_queue, enqueue_ops))\n",
    "            example_serialized = examples_queue.dequeue()\n",
    "    else:\n",
    "        reader = dataset.reader()\n",
    "        _, example_serialized = reader.read(filename_queue)\n",
    "\n",
    "    images_and_labels = []\n",
    "    for thread_id in range(num_preprocess_threads):\n",
    "        # Parse a serialized Example proto to extract the image and metadata.\n",
    "        image_buffer, label_index, _ = parse_example_proto(\n",
    "          example_serialized)\n",
    "        image = image_preprocessing(image_buffer, train, thread_id)\n",
    "        images_and_labels.append([image, label_index])\n",
    "\n",
    "        images, label_index_batch = tf.train.batch_join(\n",
    "                images_and_labels,\n",
    "                batch_size=batch_size,\n",
    "                capacity=2 * num_preprocess_threads * batch_size)\n",
    "\n",
    "    # Reshape images into these desired dimensions.\n",
    "    height = 224\n",
    "    width = 224\n",
    "    depth = 3\n",
    "\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = tf.reshape(images, shape=[batch_size, height, width, depth])\n",
    "\n",
    "    # Display the training images in the visualizer.\n",
    "    tf.image_summary('images', images)\n",
    "\n",
    "    return images, tf.reshape(label_index_batch, [batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126\n",
      " 127 126 126 127 127 127 127 127 127 127 127 127 127 127 127 127 127 128\n",
      " 128 128 127 128 128 128 128 128 128 128 128 128 128 128 128 128 128 129\n",
      " 129 128 128 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129\n",
      " 130 129 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n",
      " 130 131 130 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131\n",
      " 131 131 131 131 132 132 132 132 132 132 132 132 132 132 132 132 132 132\n",
      " 132 132]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset('train')\n",
    "eval_dataset = Dataset('eval')\n",
    "batch_size = 128\n",
    "num_preprocess_threads = 4\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "\n",
    "    with sess.as_default():\n",
    "        with tf.device('cpu:0'):\n",
    "            train_images, train_labels = batch_inputs(train_dataset, batch_size, \n",
    "                                                      True, num_preprocess_threads, 4)\n",
    "            eval_images, eval_labels = batch_inputs(eval_dataset, batch_size, \n",
    "                                                    False, num_preprocess_threads, 1)\n",
    "\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            tf.train.start_queue_runners(sess)\n",
    "\n",
    "        print(eval_labels.eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
