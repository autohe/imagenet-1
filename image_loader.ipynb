{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "import threading\n",
    "import string\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class JpegCoder(object):\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self._cmyk_data = tf.placeholder(dtype=tf.string)\n",
    "        image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n",
    "        self._cmyk_to_rgb = tf.image.encode_jpeg(image, format='rgb', quality=100)\n",
    "\n",
    "        self.decode_jpeg_data = tf.placeholder(dtype = tf.string)\n",
    "        self.decode_jpeg = tf.image.decode_jpeg(self.decode_jpeg_data, channels = 3)\n",
    "        \n",
    "    def decoder_jpeg(self, image_data):\n",
    "        image = self.sess.run(self.decode_jpeg, feed_dict = {self.decode_jpeg_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "    \n",
    "    def cmyk_to_rgb(self, image_data):\n",
    "        return self.sess.run(self._cmyk_to_rgb,\n",
    "                          feed_dict={self._cmyk_data: image_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _is_cmyk(filename):\n",
    "    \"\"\"Determine if file contains a CMYK JPEG format image.\n",
    "    Args:\n",
    "    filename: string, path of the image file.\n",
    "    Returns:\n",
    "    boolean indicating if the image is a JPEG encoded with CMYK color space.\n",
    "    \"\"\"\n",
    "    # File list from:\n",
    "    # https://github.com/cytsai/ilsvrc-cmyk-image-list\n",
    "    blacklist = ['n01739381_1309.JPEG', 'n02077923_14822.JPEG',\n",
    "               'n02447366_23489.JPEG', 'n02492035_15739.JPEG',\n",
    "               'n02747177_10752.JPEG', 'n03018349_4028.JPEG',\n",
    "               'n03062245_4620.JPEG', 'n03347037_9675.JPEG',\n",
    "               'n03467068_12171.JPEG', 'n03529860_11437.JPEG',\n",
    "               'n03544143_17228.JPEG', 'n03633091_5218.JPEG',\n",
    "               'n03710637_5125.JPEG', 'n03961711_5286.JPEG',\n",
    "               'n04033995_2932.JPEG', 'n04258138_17003.JPEG',\n",
    "               'n04264628_27969.JPEG', 'n04336792_7448.JPEG',\n",
    "               'n04371774_5854.JPEG', 'n04596742_4225.JPEG',\n",
    "               'n07583066_647.JPEG', 'n13037406_4650.JPEG']\n",
    "    return filename.split('/')[-1] in blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(filename, coder):\n",
    "    print(filename)\n",
    "    image_data = tf.gfile.FastGFile(filename, 'rb').read()\n",
    "    \n",
    "    if _is_cmyk(filename):\n",
    "        image_data = coder.cmyk_to_rgb(image_data)\n",
    "    \n",
    "    image = coder.decoder_jpeg(image_data)\n",
    "    \n",
    "    assert len(image.shape) == 3\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    assert image.shape[2] ==3\n",
    "    \n",
    "    return image_data, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_image_batch(coder, name, thread_index, ranges, synsets, filenames, labels, num_shards):\n",
    "    num_threads = len(ranges)\n",
    "    assert not num_shards % num_threads\n",
    "    shards_in_batch = int(num_shards / num_threads)\n",
    "    \n",
    "    shard_ranges = np.linspace(ranges[thread_index][0], \n",
    "                               ranges[thread_index][1], shards_in_batch + 1).astype(int)\n",
    "    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(shards_in_batch):\n",
    "        shard = thread_index * shards_in_batch + i\n",
    "        output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)\n",
    "        output_file = os.path.join('/data/ImageNet/output_data/', output_filename)\n",
    "        writer = tf.python_io.TFRecordWriter(output_file)\n",
    "        \n",
    "        shard_counter = 0\n",
    "        files_in_shard = np.arange(shard_ranges[i], shard_ranges[i+1], dtype=int)\n",
    "        for j in files_in_shard:\n",
    "\n",
    "            filename = filenames[j]\n",
    "            label = labels[j]\n",
    "            \n",
    "            image_buffer, height, width = process_image(filename, coder)\n",
    "            \n",
    "            colorspace = b'RGB'\n",
    "            channels = 3\n",
    "            image_format = b'JPEG'\n",
    "            basename = str.encode(os.path.basename(filename))\n",
    "            \n",
    "            example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "                'image/height': _int64_feature(height),\n",
    "                'image/width': _int64_feature(width),\n",
    "                'image/colorspace': _bytes_feature(colorspace),\n",
    "                'image/channels': _int64_feature(channels),\n",
    "                'image/class/label': _int64_feature(label),\n",
    "                'image/class/synsets': _bytes_featre(synset),\n",
    "                'image/format': _bytes_feature(image_format),\n",
    "                'image/filename': _bytes_feature(basename),\n",
    "                'image/encoded': _bytes_feature(image_buffer)}))\n",
    "                        \n",
    "              \n",
    "            writer.write(example.SerializeToString())\n",
    "            shard_counter += 1\n",
    "            counter += 1\n",
    "            if not counter % 1000:\n",
    "                sys.stdout.flush()\n",
    "        sys.stdout.flush()    \n",
    "        shard_counter = 0\n",
    "    sys.stdout.flush()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "synsets = []\n",
    "enc = 'utf-8'\n",
    "\n",
    "f = open('/data/ImageNet/dev_kit/traindata_labels.txt', 'r')\n",
    "\n",
    "synsets = f.readlines()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_index = 1\n",
    "labels = []\n",
    "synsets  = []\n",
    "filenames = []\n",
    "\n",
    "for synset in synsets:\n",
    "    synset = synset.strip()\n",
    "    jpeg_path = '/data/ImageNet/train_data/%s/*.JPEG' % synset\n",
    "    \n",
    "    matching_files = glob.glob(jpeg_path)\n",
    "    \n",
    "    labels.extend([label_index] * len(matching_files))\n",
    "    synsets.extend([synset] * len(mathcing_files))\n",
    "    filenames.extend(matching_files)\n",
    "    \n",
    "    label_index += 1\n",
    "    \n",
    "shuffled_index = range(len(filenames))\n",
    "random.seed(12345)\n",
    "random.shuffle(filenames)    \n",
    "    \n",
    "filenames = [filenames[i] for i in shuffled_index]\n",
    "synsets = [synsets[i] for i in shuffled_index]\n",
    "labels = [labels[i] for i in shuffled_index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_labels = [l.strip() for l in open('/data/ImageNet/dev_kit/imagenet_2012_validation_synset_labels.txt')\n",
    "               .readlines()]\n",
    "eval_filenames = []\n",
    "\n",
    "data_dir = '/data/ImageNet/eval_data/'\n",
    "\n",
    "\n",
    "for i in range(len(eval_labels)):\n",
    "    basename = 'ILSVRC2012_val_000%.5d.JPEG' % (i + 1)\n",
    "    new_filename = os.path.join(data_dir, eval_labels[i], basename)\n",
    "\n",
    "    eval_filenames.extend(new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-137:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 868, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-78-87aa29bf5bc5>\", line 24, in process_image_batch\n",
      "    image_buffer, height, width = process_image(filename, coder)\n",
      "  File \"<ipython-input-61-2a564a69aca9>\", line 3, in process_image\n",
      "    image_data = tf.gfile.FastGFile(filename, 'rb').read()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/gfile.py\", line 222, in __init__\n",
      "    super(FastGFile, self).__init__(name, mode, _Nulllocker())\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/gfile.py\", line 63, in __init__\n",
      "    self._fp = open(name, mode)\n",
      "IsADirectoryError: [Errno 21] Is a directory: '/'\n",
      "\n",
      "Exception in thread Thread-138:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 868, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-78-87aa29bf5bc5>\", line 22, in process_image_batch\n",
      "    label = labels[j]\n",
      "IndexError: list index out of range\n",
      "\n",
      "Exception in thread Thread-139:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 868, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-78-87aa29bf5bc5>\", line 22, in process_image_batch\n",
      "    label = labels[j]\n",
      "IndexError: list index out of range\n",
      "\n",
      "Exception in thread Thread-140:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 868, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-78-87aa29bf5bc5>\", line 22, in process_image_batch\n",
      "    label = labels[j]\n",
      "IndexError: list index out of range\n",
      "\n",
      "Exception in thread Thread-141:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 868, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-78-87aa29bf5bc5>\", line 22, in process_image_batch\n",
      "    label = labels[j]\n",
      "IndexError: list index out of range\n",
      "\n",
      "Exception in thread Thread-142:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 868, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-78-87aa29bf5bc5>\", line 22, in process_image_batch\n",
      "    label = labels[j]\n",
      "IndexError: list index out of range\n",
      "\n",
      "Exception in thread Thread-143:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 868, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-78-87aa29bf5bc5>\", line 22, in process_image_batch\n",
      "    label = labels[j]\n",
      "IndexError: list index out of range\n",
      "\n",
      "Exception in thread Thread-144:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 868, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-78-87aa29bf5bc5>\", line 22, in process_image_batch\n",
      "    label = labels[j]\n",
      "IndexError: list index out of range\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_shards = 1024\n",
    "eval_shards = 128\n",
    "num_threads = 8\n",
    "\n",
    "spacing = np.linspace(0, len(filenames), num_threads + 1).astype(np.int)\n",
    "ranges = []\n",
    "threads = []\n",
    "for j in range(len(spacing) - 1):\n",
    "    ranges.append([spacing[j], spacing[j+1]])\n",
    "\n",
    "# launch thread\n",
    "sys.stdout.flush()\n",
    "\n",
    "# monitoring\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "coder = JpegCoder()\n",
    "\n",
    "threads = []\n",
    "for thread_index in range(len(ranges)):\n",
    "    args = (coder, 'train', thread_index, ranges, synsets, filenames, labels, train_shards)\n",
    "    t = threading.Thread(target = process_image_batch, args=args)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "    coord.join(threads)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "spacing = np.linspace(0, len(eval_filenames), num_threads + 1).astype(np.int)\n",
    "ranges = []\n",
    "threads = []\n",
    "for j in range(len(spacing) - 1):\n",
    "    ranges.append([spacing[j], spacing[j+1]])\n",
    "\n",
    "# launch thread\n",
    "sys.stdout.flush()\n",
    "\n",
    "# monitoring\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "coder = JpegCoder()\n",
    "\n",
    "threads = []\n",
    "for thread_index in range(len(ranges)):\n",
    "    args = (coder, 'eval', thread_index, ranges, synsets, eval_filenames, eval_labels, eval_shards)\n",
    "    t = threading.Thread(target = process_image_batch, args=args)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "    coord.join(threads)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = []\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta\n",
    "from abc import abstractmethod\n",
    "\n",
    "class Dataset(object):\n",
    "    \"\"\"A simple class for handling data sets.\"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self,  subset):\n",
    "        \"\"\"Initialize dataset using a subset and the path to the data.\"\"\"\n",
    "        assert subset in self.available_subsets(), self.available_subsets()\n",
    "        self.subset = subset\n",
    "\n",
    "    @abstractmethod\n",
    "    def num_classes(self):\n",
    "        return 1000\n",
    "\n",
    "    @abstractmethod\n",
    "    def num_examples_per_epoch(self):\n",
    "    \"\"\"Returns the number of examples in the data subset.\"\"\"\n",
    "    if self.subset == 'train':\n",
    "        return 1281167\n",
    "    if self.subset == 'eval':\n",
    "        return 50000\n",
    "\n",
    "    @abstractmethod\n",
    "\n",
    "\n",
    "    def available_subsets(self):\n",
    "        \"\"\"Returns the list of available subsets.\"\"\"\n",
    "        return ['train', 'validation']\n",
    "\n",
    "    def data_files(self):\n",
    "        \"\"\"Returns a python list of all (sharded) data subset files.\n",
    "        Returns:\n",
    "          python list of all (sharded) data set files.\n",
    "        Raises:\n",
    "          ValueError: if there are not data_files matching the subset.\n",
    "        \"\"\"\n",
    "        tf_record_pattern = os.path.join('/data/ImageNet/output_data/', '%s-*' % self.subset)\n",
    "        data_files = tf.gfile.Glob(tf_record_pattern)\n",
    "        if not data_files:\n",
    "          print('No files found for dataset %s at %s' % (\n",
    "                                                            self.subset,\n",
    "                                                        FLAGS.data_dir))\n",
    "\n",
    "          self.download_message()\n",
    "          exit(-1)\n",
    "        return data_files\n",
    "\n",
    "    def reader(self):\n",
    "        \"\"\"Return a reader for a single entry from the data set.\n",
    "        See io_ops.py for details of Reader class.\n",
    "        Returns:\n",
    "          Reader object that reads the data set.\n",
    "        \"\"\"\n",
    "        return tf.TFRecordReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_example_proto(example_serialized):\n",
    "    \"\"\"Parses an Example proto containing a training example of an image.\n",
    "    The output of the build_image_data.py image preprocessing script is a dataset\n",
    "    containing serialized Example protocol buffers. Each Example proto contains\n",
    "    the following fields:\n",
    "    image/height: 462\n",
    "    image/width: 581\n",
    "    image/colorspace: 'RGB'\n",
    "    image/channels: 3\n",
    "    image/class/label: 615\n",
    "\n",
    "\n",
    "    image/format: 'JPEG'\n",
    "    image/filename: 'ILSVRC2012_val_00041207.JPEG'\n",
    "    image/encoded: <JPEG encoded string>\n",
    "    Args:\n",
    "    example_serialized: scalar Tensor tf.string containing a serialized\n",
    "      Example protocol buffer.\n",
    "    Returns:\n",
    "    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n",
    "    label: Tensor tf.int32 containing the label.\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged as\n",
    "      [ymin, xmin, ymax, xmax].\n",
    "    text: Tensor tf.string containing the human-readable label.\n",
    "    \"\"\"\n",
    "    # Dense features in Example proto.\n",
    "    feature_map = {\n",
    "      'image/encoded': tf.FixedLenFeature([], dtype=tf.string,\n",
    "                                          default_value=''),\n",
    "      'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64,\n",
    "                                              default_value=-1),\n",
    "\n",
    "    }\n",
    "    sparse_float32 = tf.VarLenFeature(dtype=tf.float32)\n",
    "    # Sparse features in Example proto.\n",
    "    feature_map.update(\n",
    "      {k: sparse_float32 for k in ['image/object/bbox/xmin',\n",
    "                                   'image/object/bbox/ymin',\n",
    "                                   'image/object/bbox/xmax',\n",
    "                                   'image/object/bbox/ymax']})\n",
    "\n",
    "    features = tf.parse_single_example(example_serialized, feature_map)\n",
    "    label = tf.cast(features['image/class/label'], dtype=tf.int32)\n",
    "\n",
    "\n",
    "    # Note that we impose an ordering of (y, x) just to make life difficult.\n",
    "    bbox = tf.concat(0, [ymin, xmin, ymax, xmax])\n",
    "\n",
    "    # Force the variable number of bounding boxes into the shape\n",
    "    # [1, num_boxes, coords].\n",
    "    bbox = tf.expand_dims(bbox, 0)\n",
    "    bbox = tf.transpose(bbox, [0, 2, 1])\n",
    "\n",
    "    return features['image/encoded'], label, bbox, features['image/class/text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_inputs(dataset, batch_size, train, num_preprocess_threads, num_readers):\n",
    "    data_files = dataset.data_files()\n",
    "    if data_files is None:\n",
    "        raise ValueError('No data files found for this dataset')\n",
    "\n",
    "    # Create filename_queue\n",
    "    if train:\n",
    "        filename_queue = tf.train.string_input_producer(data_files,\n",
    "                                                      shuffle=True,\n",
    "                                                      capacity=16)\n",
    "    else:\n",
    "        filename_queue = tf.train.string_input_producer(data_files,\n",
    "                                                      shuffle=False,\n",
    "                                                      capacity=1)\n",
    "    if num_preprocess_threads is None:\n",
    "        num_preprocess_threads = FLAGS.num_preprocess_threads\n",
    "\n",
    "    if num_preprocess_threads % 4:\n",
    "        raise ValueError('Please make num_preprocess_threads a multiple '\n",
    "                       'of 4 (%d % 4 != 0).', num_preprocess_threads)\n",
    "\n",
    "    if num_readers is None:\n",
    "        num_readers = FLAGS.num_readers\n",
    "\n",
    "    if num_readers < 1:\n",
    "        raise ValueError('Please make num_readers at least 1')\n",
    "        \n",
    "    examples_per_shard = 1024\n",
    "    \n",
    "    min_queue_examples = examples_per_shard * 16\n",
    "    if train:\n",
    "        examples_queue = tf.RandomShuffleQueue(\n",
    "              capacity=min_queue_examples + 3 * batch_size,\n",
    "              min_after_dequeue=min_queue_examples,\n",
    "              dtypes=[tf.string])\n",
    "    else:\n",
    "        examples_queue = tf.FIFOQueue(\n",
    "              capacity=examples_per_shard + 3 * batch_size,\n",
    "              dtypes=[tf.string])\n",
    "\n",
    "    # Create multiple readers to populate the queue of examples.\n",
    "    if num_readers > 1:\n",
    "        enqueue_ops = []\n",
    "        for _ in range(num_readers):\n",
    "            reader = dataset.reader()\n",
    "            _, value = reader.read(filename_queue)\n",
    "            enqueue_ops.append(examples_queue.enqueue([value]))\n",
    "\n",
    "            tf.train.queue_runner.add_queue_runner(\n",
    "                    tf.train.queue_runner.QueueRunner(examples_queue, enqueue_ops))\n",
    "            example_serialized = examples_queue.dequeue()\n",
    "    else:\n",
    "        reader = dataset.reader()\n",
    "        _, example_serialized = reader.read(filename_queue)\n",
    "\n",
    "    images_and_labels = []\n",
    "    for thread_id in range(num_preprocess_threads):\n",
    "        # Parse a serialized Example proto to extract the image and metadata.\n",
    "        image_buffer, label_index, bbox, _ = parse_example_proto(\n",
    "          example_serialized)\n",
    "        image = image_preprocessing(image_buffer, bbox, train, thread_id)\n",
    "        images_and_labels.append([image, label_index])\n",
    "\n",
    "        images, label_index_batch = tf.train.batch_join(\n",
    "                images_and_labels,\n",
    "                batch_size=batch_size,\n",
    "                capacity=2 * num_preprocess_threads * batch_size)\n",
    "\n",
    "    # Reshape images into these desired dimensions.\n",
    "    height = 28\n",
    "    width = 28\n",
    "    depth = 3\n",
    "\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = tf.reshape(images, shape=[batch_size, height, width, depth])\n",
    "\n",
    "    # Display the training images in the visualizer.\n",
    "    tf.image_summary('images', images)\n",
    "\n",
    "    return images, tf.reshape(label_index_batch, [batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset('train')\n",
    "eval_dataset = Dataset('eval')\n",
    "batch_size = 128\n",
    "num_preprocess_threads = 4\n",
    "with tf.device('cpu:0'):\n",
    "    train_images, train_labels = batch_inputs(train_dataset, batch_size, \n",
    "                                              train = True, num_preprocess_threads, 4)\n",
    "    eval_images, eval_labels = batch_inputs(eval_dataset, batch_size, \n",
    "                                            train = False, num_preprocess_threads, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
