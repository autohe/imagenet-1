{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Wrapper for inserting float features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _convert_to_example(filename, image_buffer, label, synset, human,\n",
    "                        height, width):\n",
    "    \"\"\"Build an Example proto for an example.\n",
    "\n",
    "    Args:\n",
    "    filename: string, path to an image file, e.g., '/path/to/example.JPG'\n",
    "    image_buffer: string, JPEG encoding of RGB image\n",
    "    label: integer, identifier for the ground truth for the network\n",
    "    synset: string, unique WordNet ID specifying the label, e.g., 'n02323233'\n",
    "    human: string, human-readable label, e.g., 'red fox, Vulpes vulpes'\n",
    "    bbox: list of bounding boxes; each box is a list of integers\n",
    "      specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong to\n",
    "      the same label as the image label.\n",
    "    height: integer, image height in pixels\n",
    "    width: integer, image width in pixels\n",
    "    Returns:\n",
    "    Example proto\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    colorspace = 'RGB'\n",
    "    channels = 3\n",
    "    image_format = 'JPEG'\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/height': _int64_feature(height),\n",
    "      'image/width': _int64_feature(width),\n",
    "      'image/colorspace': _bytes_feature(colorspace),\n",
    "      'image/channels': _int64_feature(channels),\n",
    "      'image/class/label': _int64_feature(label),\n",
    "      'image/class/synset': _bytes_feature(synset),\n",
    "      'image/class/text': _bytes_feature(human),\n",
    "      'image/format': _bytes_feature(image_format),\n",
    "      'image/filename': _bytes_feature(os.path.basename(filename)),\n",
    "      'image/encoded': _bytes_feature(image_buffer)}))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ImageCoder(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # Initializes function that converts CMYK JPEG data to RGB JPEG data.\n",
    "        self._cmyk_data = tf.placeholder(dtype=tf.string)\n",
    "        image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n",
    "        self._cmyk_to_rgb = tf.image.encode_jpeg(image, format='rgb', quality=100)\n",
    "\n",
    "        # Initializes function that decodes RGB JPEG data.\n",
    "        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def cmyk_to_rgb(self, image_data):\n",
    "        return self._sess.run(self._cmyk_to_rgb,\n",
    "                              feed_dict={self._cmyk_data: image_data})\n",
    "\n",
    "    \n",
    "    def decode_jpeg(self, image_data):\n",
    "        image = self._sess.run(self._decode_jpeg,\n",
    "                               feed_dict={self._decode_jpeg_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _is_cmyk(filename):\n",
    "    \"\"\"Determine if file contains a CMYK JPEG format image.\n",
    "    Args:\n",
    "    filename: string, path of the image file.\n",
    "    Returns:\n",
    "    boolean indicating if the image is a JPEG encoded with CMYK color space.\n",
    "    \"\"\"\n",
    "    # File list from:\n",
    "    # https://github.com/cytsai/ilsvrc-cmyk-image-list\n",
    "    blacklist = ['n01739381_1309.JPEG', 'n02077923_14822.JPEG',\n",
    "               'n02447366_23489.JPEG', 'n02492035_15739.JPEG',\n",
    "               'n02747177_10752.JPEG', 'n03018349_4028.JPEG',\n",
    "               'n03062245_4620.JPEG', 'n03347037_9675.JPEG',\n",
    "               'n03467068_12171.JPEG', 'n03529860_11437.JPEG',\n",
    "               'n03544143_17228.JPEG', 'n03633091_5218.JPEG',\n",
    "               'n03710637_5125.JPEG', 'n03961711_5286.JPEG',\n",
    "               'n04033995_2932.JPEG', 'n04258138_17003.JPEG',\n",
    "               'n04264628_27969.JPEG', 'n04336792_7448.JPEG',\n",
    "               'n04371774_5854.JPEG', 'n04596742_4225.JPEG',\n",
    "               'n07583066_647.JPEG', 'n13037406_4650.JPEG']\n",
    "    return filename.split('/')[-1] in blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_image(filename, coder):\n",
    "    \"\"\"Process a single image file.\n",
    "    Args:\n",
    "    filename: string, path to an image file e.g., '/path/to/example.JPG'.\n",
    "    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n",
    "    Returns:\n",
    "    image_buffer: string, JPEG encoding of RGB image.\n",
    "    height: integer, image height in pixels.\n",
    "    width: integer, image width in pixels.\n",
    "    \"\"\"\n",
    "    # Read the image file.\n",
    "    image_data = tf.gfile.FastGFile(filename, 'r').read()\n",
    "\n",
    "    # Clean the dirty data\n",
    "    if _is_cmyk(filename):\n",
    "        # 22 JPEG images are in CMYK colorspace.\n",
    "        print('Converting CMYK to RGB for %s' % filename)\n",
    "        image_data = coder.cmyk_to_rgb(image_data)\n",
    "\n",
    "    # Decode the RGB JPEG.\n",
    "    image = coder.decode_jpeg(image_data)\n",
    "\n",
    "    # Check that image converted to RGB\n",
    "    assert len(image.shape) == 3\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    assert image.shape[2] == 3\n",
    "\n",
    "    return image_data, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-256-8c0071c13e7d>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-256-8c0071c13e7d>\"\u001b[0;36m, line \u001b[0;32m52\u001b[0m\n\u001b[0;31m    height, width)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n",
    "                               synsets, labels, humans, num_shards):\n",
    "    \"\"\"Processes and saves list of images as TFRecord in 1 thread.\n",
    "\n",
    "    Args:\n",
    "    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n",
    "    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n",
    "    ranges: list of pairs of integers specifying ranges of each batches to\n",
    "      analyze in parallel.\n",
    "    name: string, unique identifier specifying the data set\n",
    "    filenames: list of strings; each string is a path to an image file\n",
    "    synsets: list of strings; each string is a unique WordNet ID\n",
    "    labels: list of integer; each integer identifies the ground truth\n",
    "    humans: list of strings; each string is a human-readable label\n",
    "    bboxes: list of bounding boxes for each image. Note that each entry in this\n",
    "      list might contain from 0+ entries corresponding to the number of bounding\n",
    "      box annotations for the image.\n",
    "    num_shards: integer number of shards for this data set.\n",
    "    \"\"\"\n",
    "    # Each thread produces N shards where N = int(num_shards / num_threads).\n",
    "    # For instance, if num_shards = 128, and the num_threads = 2, then the first\n",
    "    # thread would produce shards [0, 64).\n",
    "    num_threads = len(ranges)\n",
    "    assert not num_shards % num_threads\n",
    "    num_shards_per_batch = int(num_shards / num_threads)\n",
    "\n",
    "    shard_ranges = np.linspace(ranges[thread_index][0],\n",
    "                             ranges[thread_index][1],\n",
    "                             num_shards_per_batch + 1).astype(int)\n",
    "    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n",
    "\n",
    "    counter = 0\n",
    "    for s in xrange(num_shards_per_batch):\n",
    "        # Generate a sharded version of the file name, e.g. 'train-00002-of-00010'\n",
    "        shard = thread_index * num_shards_per_batch + s\n",
    "        output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)\n",
    "        output_file = os.path.join(FLAGS.output_directory, output_filename)\n",
    "        writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "        shard_counter = 0\n",
    "        files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n",
    "        for i in files_in_shard:\n",
    "            filename = filenames[i]\n",
    "            label = labels[i]\n",
    "            synset = synsets[i]\n",
    "            human = humans[i]\n",
    "\n",
    "            image_buffer, height, width = _process_image(filename, coder)\n",
    "\n",
    "            example = _convert_to_example(filename, image_buffer, label,\n",
    "                                        synset, human\n",
    "                                        height, width)\n",
    "            writer.write(example.SerializeToString())\n",
    "            shard_counter += 1\n",
    "            counter += 1\n",
    "\n",
    "            if not counter % 1000:\n",
    "                print('%s [thread %d]: Processed %d of %d images in thread batch.' %\n",
    "                      (datetime.now(), thread_index, counter, num_files_in_thread))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        print('%s [thread %d]: Wrote %d images to %s' %\n",
    "            datetime.now(), thread_index, shard_counter, output_file)\n",
    "        sys.stdout.flush()\n",
    "        shard_counter = 0\n",
    "    print('%s [thread %d]: Wrote %d images to %d shards.' %\n",
    "        (datetime.now(), thread_index, counter, num_files_in_thread))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _process_image_files(name, filenames, synsets, labels, humans, num_shards):\n",
    "    \"\"\"Process and save list of images as TFRecord of Example protos.\n",
    "\n",
    "  Args:\n",
    "    name: string, unique identifier specifying the data set\n",
    "    filenames: list of strings; each string is a path to an image file\n",
    "    synsets: list of strings; each string is a unique WordNet ID\n",
    "    labels: list of integer; each integer identifies the ground truth\n",
    "    humans: list of strings; each string is a human-readable label\n",
    "    bboxes: list of bounding boxes for each image. Note that each entry in this\n",
    "      list might contain from 0+ entries corresponding to the number of bounding\n",
    "      box annotations for the image.\n",
    "    num_shards: integer number of shards for this data set.\n",
    "    \"\"\"\n",
    "    assert len(filenames) == len(synsets)\n",
    "    assert len(filenames) == len(labels)\n",
    "    assert len(filenames) == len(humans)\n",
    "    assert len(filenames) == len(bboxes)\n",
    "\n",
    "    # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n",
    "    spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n",
    "    ranges = []\n",
    "    threads = []\n",
    "    for i in xrange(len(spacing) - 1):\n",
    "        ranges.append([spacing[i], spacing[i+1]])\n",
    "\n",
    "    # Launch a thread for each batch.\n",
    "    print('Launching %d threads for spacings: %s' % (FLAGS.num_threads, ranges))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Create a mechanism for monitoring when all threads are finished.\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    # Create a generic TensorFlow-based utility for converting all image codings.\n",
    "    coder = ImageCoder()\n",
    "\n",
    "    threads = []\n",
    "    for thread_index in xrange(len(ranges)):\n",
    "        args = (coder, thread_index, ranges, name, filenames,\n",
    "                synsets, labels, humans, num_shards)\n",
    "        t = threading.Thread(target=_process_image_files_batch, args=args)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # Wait for all the threads to terminate.\n",
    "    coord.join(threads)\n",
    "    print('%s: Finished writing all %d images in data set.' %\n",
    "        (datetime.now(), len(filenames)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _find_image_files(data_dir, labels_file):\n",
    "    \"\"\"Build a list of all images files and labels in the data set.\n",
    "    Args:\n",
    "    data_dir: string, path to the root directory of images.\n",
    "      Assumes that the ImageNet data set resides in JPEG files located in\n",
    "      the following directory structure.\n",
    "        data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n",
    "        data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n",
    "      where 'n01440764' is the unique synset label associated with these images.\n",
    "    labels_file: string, path to the labels file.\n",
    "      The list of valid labels are held in this file. Assumes that the file\n",
    "      contains entries as such:\n",
    "        n01440764\n",
    "        n01443537\n",
    "        n01484850\n",
    "      where each line corresponds to a label expressed as a synset. We map\n",
    "      each synset contained in the file to an integer (based on the alphabetical\n",
    "      ordering) starting with the integer 1 corresponding to the synset\n",
    "      contained in the first line.\n",
    "      The reason we start the integer labels at 1 is to reserve label 0 as an\n",
    "      unused background class.\n",
    "  Returns:\n",
    "    filenames: list of strings; each string is a path to an image file.\n",
    "    synsets: list of strings; each string is a unique WordNet ID.\n",
    "    labels: list of integer; each integer identifies the ground truth.\n",
    "  \"\"\"\n",
    "    print('Determining list of input files and labels from %s.' % data_dir)\n",
    "    challenge_synsets = [l.strip() for l in\n",
    "                       tf.gfile.FastGFile(labels_file, 'r').readlines()]\n",
    "\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    synsets = []\n",
    "\n",
    "    # Leave label index 0 empty as a background class.\n",
    "    label_index = 1\n",
    "\n",
    "    # Construct the list of JPEG files and labels.\n",
    "    for synset in challenge_synsets:\n",
    "        jpeg_file_path = '%s/%s/*.JPEG' % (data_dir, synset)\n",
    "        matching_files = tf.gfile.Glob(jpeg_file_path)\n",
    "\n",
    "        labels.extend([label_index] * len(matching_files))\n",
    "        synsets.extend([synset] * len(matching_files))\n",
    "        filenames.extend(matching_files)\n",
    "\n",
    "        if not label_index % 100:\n",
    "            print('Finished finding files in %d of %d classes.' % (\n",
    "            label_index, len(challenge_synsets)))\n",
    "        label_index += 1\n",
    "\n",
    "    # Shuffle the ordering of all image files in order to guarantee\n",
    "    # random ordering of the images with respect to label in the\n",
    "    # saved TFRecord files. Make the randomization repeatable.\n",
    "    shuffled_index = range(len(filenames))\n",
    "    random.seed(12345)\n",
    "    random.shuffle(shuffled_index)\n",
    "\n",
    "    filenames = [filenames[i] for i in shuffled_index]\n",
    "    synsets = [synsets[i] for i in shuffled_index]\n",
    "    labels = [labels[i] for i in shuffled_index]\n",
    "\n",
    "    print('Found %d JPEG files across %d labels inside %s.' %\n",
    "            (len(filenames), len(challenge_synsets), data_dir))\n",
    "    return filenames, synsets, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _find_human_readable_labels(synsets, synset_to_human):\n",
    "    humans = []\n",
    "    for s in synsets:\n",
    "        assert s in synset_to_human, ('Failed to find: %s' % s)\n",
    "        humans.append(synset_to_human[s])\n",
    "    return humans\n",
    "\n",
    "\n",
    "def _find_image_bounding_boxes(filenames, image_to_bboxes):\n",
    "\n",
    "    num_image_bbox = 0\n",
    "    bboxes = []\n",
    "    for f in filenames:\n",
    "        basename = os.path.basename(f)\n",
    "        if basename in image_to_bboxes:\n",
    "            bboxes.append(image_to_bboxes[basename])\n",
    "            num_image_bbox += 1\n",
    "        else:\n",
    "            bboxes.append([])\n",
    "    print('Found %d images with bboxes out of %d images' % (\n",
    "          num_image_bbox, len(filenames)))\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _process_dataset(name, directory, num_shards, synset_to_human):\n",
    "    filenames, synsets, labels = _find_image_files(directory, '/data/ImageNet/dev_kit/traindata_labels.TXT')\n",
    "    humans = _find_human_readable_labels(synsets, synset_to_human)\n",
    "    _process_image_files(name, filenames, synsets, labels,\n",
    "                       humans, num_shards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _build_synset_lookup(imagenet_metadata_file):\n",
    "    \"\"\"Build lookup for synset to human-readable label.\n",
    "\n",
    "    Args:\n",
    "    imagenet_metadata_file: string, path to file containing mapping from\n",
    "      synset to human-readable label.\n",
    "\n",
    "      Assumes each line of the file looks like:\n",
    "\n",
    "        n02119247    black fox\n",
    "        n02119359    silver fox\n",
    "        n02119477    red fox, Vulpes fulva\n",
    "\n",
    "      where each line corresponds to a unique mapping. Note that each line is\n",
    "      formatted as <synset>\\t<human readable label>.\n",
    "\n",
    "    Returns:\n",
    "    Dictionary of synset to human labels, such as:\n",
    "      'n02119022' --> 'red fox, Vulpes vulpes'\n",
    "    \"\"\"\n",
    "    lines = tf.gfile.FastGFile(imagenet_metadata_file, 'r').readlines()\n",
    "    print(lines)\n",
    "    synset_to_human = {}\n",
    "    for l in lines:\n",
    "        if l:\n",
    "            parts = l.strip().split('\\t')\n",
    "            assert len(parts) == 2\n",
    "            synset = parts[0]\n",
    "            human = parts[1]\n",
    "            synset_to_human[synset] = human\n",
    "    return synset_to_human\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Determining list of input files and labels from /data/ImageNet/train_data/.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/ImageNet/dev_kit/traindata_labels.TXT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-285-10f540b16907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msynset_to_human\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_synset_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/ImageNet/meta_data/metadata_labels.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_process_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/data/ImageNet/train_data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset_to_human\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-283-abf5a1fb87c3>\u001b[0m in \u001b[0;36m_process_dataset\u001b[0;34m(name, directory, num_shards, synset_to_human)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_process_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset_to_human\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_image_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/data/ImageNet/dev_kit/traindata_labels.TXT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mhumans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_human_readable_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset_to_human\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     _process_image_files(name, filenames, synsets, labels,\n\u001b[1;32m      5\u001b[0m                        humans, num_shards)\n",
      "\u001b[0;32m<ipython-input-261-fc0010d08f4f>\u001b[0m in \u001b[0;36m_find_image_files\u001b[0;34m(data_dir, labels_file)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Determining list of input files and labels from %s.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     challenge_synsets = [l.strip() for l in\n\u001b[0;32m---> 29\u001b[0;31m                        tf.gfile.FastGFile(labels_file, 'r').readlines()]\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/gfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastGFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Nulllocker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/gfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, locker)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_locker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/ImageNet/dev_kit/traindata_labels.TXT'"
     ]
    }
   ],
   "source": [
    "synset_to_human = _build_synset_lookup('/data/ImageNet/meta_data/metadata_labels.txt')\n",
    "_process_dataset('train', '/data/ImageNet/train_data/', 1024, synset_to_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
